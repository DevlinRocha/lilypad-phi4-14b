#!/usr/bin/env bash

# Initialize default values
PROMPT=""
TEMPERATURE="0.7"
MAX_TOKENS="2048"

# Create output directory if it doesn't exist
mkdir -p /outputs

echo "Input: $1" >&2

# Parse command-line arguments
while [[ $# -gt 0 ]]; do
    case "$1" in
    --prompt)
        PROMPT="$2"
        shift 2
        ;;
    --temperature)
        TEMPERATURE="$2"
        shift 2
        ;;
    --max_tokens)
        MAX_TOKENS="$2"
        shift 2
        ;;
    *)
        echo "Unknown flag: $1" >&2
        exit 1
        ;;
    esac
done

# Extract values from input JSON with defaults
messages="[{\"role\": \"user\", \"content\": \"$PROMPT\"}]"

# Start the ollama server in the background
echo "Starting Ollama server..." >&2
nohup bash -c "ollama serve &" >&2

# Wait for server with timeout
timeout=30
start_time=$(date +%s)
while ! curl -s http://127.0.0.1:11434 >/dev/null; do
    current_time=$(date +%s)
    elapsed=$((current_time - start_time))
    if [ $elapsed -gt $timeout ]; then
        echo "Timeout waiting for Ollama server" >&2
        exit 1
    fi
    echo "Waiting for ollama to start... ($elapsed seconds)" >&2
    sleep 1
done

echo "Ollama server started" >&2

# Prepare the chat completion request
request=$(
    cat <<EOF
{
  "model": "$MODEL_ID",
  "messages": $messages,
  "temperature": $TEMPERATURE,
  "max_tokens": $MAX_TOKENS,
  "stream": false
}
EOF
)

# Make the API call to Ollama's chat endpoint
echo "Making request to Ollama..." >&2
response=$(curl -s http://127.0.0.1:11434/api/chat \
    -H "Content-Type: application/json" \
    -d "$request")

# Create JSON structure following OpenAI format
escaped_response=$(echo "$response" | sed 's/"/\\"/g')
formatted_response="{
    \"id\": \"cmpl-$(openssl rand -hex 12)\",
    \"object\": \"text_completion\",
    \"created\": "$(date +%s)",
    \"model\": \"$MODEL_ID\",
    \"choices\": [{
        \"text\": \"$escaped_response\",
        \"index\": 0,
        \"logprobs\": null,
        \"finish_reason\": \"stop\"
    }],
    \"usage\": {
        \"prompt_tokens\": null,
        \"completion_tokens\": null,
        \"total_tokens\": null
    }
}"

# Save debug info
{
    echo "=== Debug Info ==="
    echo "Input: $1"
    echo "Request to Ollama: $request"
    echo "Response from Ollama:"
    echo "$response"
    echo "Formatted response:"
    echo "$formatted_response"
    echo "=== Server Status ==="
    echo "Ollama version: $(ollama --version)"
    echo "Model list: $(ollama list)"
    echo "Server health check: $(curl -s http://127.0.0.1:11434)"
} >"/outputs/debug.log"

# Save and output the raw Ollama response
echo "$response" >"/outputs/response.json"
echo "$response"
echo "$formatted_response" >"/outputs/formatted_response.json"
echo "$formatted_response"

exit 0
